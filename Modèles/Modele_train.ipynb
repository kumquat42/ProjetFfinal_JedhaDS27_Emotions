{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WseUWFUQlLim"
      },
      "source": [
        "# Implementation using Resnet50 de TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHWwhg-CFlRQ",
        "outputId": "2bbcd6aa-593d-4f1e-db51-e4c72a6e74f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ProjetFinal_JedhaDS27_Emotions'...\n",
            "remote: Enumerating objects: 34152, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 34152 (delta 0), reused 2 (delta 0), pack-reused 34149\u001b[K\n",
            "Receiving objects: 100% (34152/34152), 138.90 MiB | 29.21 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "Updating files: 100% (35890/35890), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/kumquat42/ProjetFinal_JedhaDS27_Emotions.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "l8gnoChwlgnx"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "import tensorflow as tf\n",
        "# from pathlib import Path\n",
        "tf.__version__\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.applications import resnet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Pur6I4ru8-"
      },
      "source": [
        "# Importation Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuexI6SWzRfm"
      },
      "source": [
        "- Data augmentation for train only\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "fl6l-8LmxJIQ"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/ProjetFinal_JedhaDS27_Emotions/Datasets/FER2013/train\"\n",
        "test_dir = \"/content/ProjetFinal_JedhaDS27_Emotions/Datasets/FER2013/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "IcZ0Ip0LB6P7",
        "outputId": "9024678b-f6a5-4f77-ff99-999edbddd9af"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=48x48>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with Image.open(f'{test_dir}/angry/PrivateTest_10131363.jpg') as img:\n",
        "#with Image.open(f'{train_dir}/angry/Training_10118481.jpg') as img:\n",
        "  display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "FINE_TUNING = 30\n",
        "\n",
        "EARLY_STOPPING_CRITERIA = 3\n",
        "EPOCHS = 30\n",
        "\n",
        "\n",
        "#  A travailler\n",
        "CLASS_LABELS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'] # A faire\n",
        "class_weight = {0: 1.,\n",
        "                1: 50.,\n",
        "                2: 2.} # -> A faire\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk1YWAIWw87W",
        "outputId": "d85b859a-135d-455c-f566-c8efe041b819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation to add a few weird stuffs for the train only\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 45,\n",
        "    width_shift_range = 0.1,        # Randomly shift the width of images by up to 10%\n",
        "    height_shift_range = 0.1,   # Randomly shift the height of images by up to 10%\n",
        "    brightness_range=(0.5, 1),\n",
        "    rescale = 1./255,\n",
        "    validation_split = 0.3\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory = train_dir,\n",
        "    target_size = (48, 48),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    color_mode = \"rgb\",     #grayscale\n",
        "    class_mode = \"categorical\",\n",
        "    shuffle=True,\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "# Validation_set à partir du test_set\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory = train_dir,\n",
        "    target_size = (48, 48),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    color_mode = \"rgb\",\n",
        "    class_mode = \"categorical\",\n",
        "    shuffle=True,\n",
        "    subset=\"validation\"\n",
        "\n",
        ")\n",
        "\n",
        "# Found 28709 images belonging to 7 classes. To update\n",
        "# Found 7178 images belonging to 7 classes. To update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIejXgrXIBfl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Not tested yet\n",
        "# voir les images - à tester...\n",
        "imgs, labels = next(iter(img_generator_flow_train))\n",
        "for img, label in zip(imgs, labels):\n",
        "  true_file_path = np.argmax(img_generator_flow_train.labels == tf.argmax(label)) # Je cherche n'importe quelle index avec une valeur True\n",
        "  true_label_name = Path(img_generator_flow_train.filepaths[true_file_path]).parent.name\n",
        "  plt.imshow(img)\n",
        "  plt.title(true_label_name)\n",
        "  plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwmEooGcGP6W"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Useless for now\n",
        "\n",
        "\n",
        "# with Image.open('/home/melb/Documents/Projects/Projet final/ProjetFinal_JedhaDS27_Emotions/FER2013/train/angry/Training_8521861.jpg') as im:\n",
        "#        im.show()\n",
        "\n",
        "array_rgb = np.array(im.convert(mode = 'RGB')).reshape(48,48,3) # ValueError: cannot reshape array of size 2304 into shape (48,48,3)\n",
        "# im_rgb = Image.fromarray(array_rgb)\n",
        "# im_rgb.show()\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDft_MzLsnFV"
      },
      "source": [
        "# models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nTgqaRrnTVZ"
      },
      "source": [
        "Import du modèle resnet avec les poids Imagenet\n",
        "\n",
        "resnet_model_1 :\n",
        "- Données : 28709 images (train), val 7178 imags (test)\n",
        "- Batch 64\n",
        "- Poids du modèle Imagenet. Je unfreeze la dernière couche\n",
        "- Paramètres:\n",
        " input_[(None, 48, 48, 3)]\n",
        " output (None, 2, 2, 2048)    \n",
        "\n",
        "Total params: 23587712 (89.98 MB)\n",
        "Trainable params: 0 (0.00 Byte)\n",
        "Non-trainable params: 23587712 (89.98 MB)\n",
        "\n",
        "class_res_1:  \n",
        "Réentrainement seulement sur la dernière couche + en dense 7  \n",
        "Paramètres:\n",
        "Total params: 23602055 (90.03 MB)\n",
        "Trainable params: 14343 (56.03 KB)\n",
        "Non-trainable params: 23587712 (89.98 MB)\n",
        "his_res_1\n",
        "\n",
        "Résultats pourris\n",
        "\n",
        "class_res_30:\n",
        "Réentrainement seulement sur les 30 dernières couches + en dense 7  \n",
        "\n",
        "Total params: 23602055 (90.03 MB)\n",
        "Trainable params: 14464519 (55.18 MB)\n",
        "Non-trainable params: 9137536 (34.86 MB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "64KkILU4lzlH"
      },
      "outputs": [],
      "source": [
        "resnet_model_1 = tf.keras.applications.resnet50.ResNet50(input_shape=(48,48,3), # ne fonctionne pas avec 48, 48, 1 car les images sont censées être en couleur...\n",
        "                                               include_top=False,\n",
        "                                               weights = \"imagenet\"\n",
        "                                               )\n",
        "\n",
        "resnet_model_1.trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjQprmN1OurQ",
        "outputId": "53f4232c-8374-4910-ef71-14b1b7c0912a"
      },
      "outputs": [],
      "source": [
        "print(len(resnet_model_1.layers)) # 175\n",
        "resnet_model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iMI7t9PIIOO5",
        "outputId": "5d8c84db-ae6e-4f98-bda4-6e32916a67b1"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(resnet_model_1, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm-6zLtvHxuu",
        "outputId": "0d1f9216-7632-47f4-af9a-aae52261ae76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 2, 2, 2048)        23587712  \n",
            "                                                                 \n",
            " global_max_pooling2d_4 (Gl  (None, 2048)              0         \n",
            " obalMaxPooling2D)                                               \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 7)                 14343     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23602055 (90.03 MB)\n",
            "Trainable params: 14464519 (55.18 MB)\n",
            "Non-trainable params: 9137536 (34.86 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Class_res_1 : on ré-entraine (30) dernière layer:\n",
        "resnet_model_1.trainable = True\n",
        "\n",
        "fine_tune_at = len(resnet_model_1.layers) - FINE_TUNING\n",
        "\n",
        "for layer in resnet_model_1.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# ajout de la couche dense:\n",
        "class_res_30 = tf.keras.Sequential([\n",
        "    resnet_model_1,\n",
        "    tf.keras.layers.GlobalMaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(7, activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "class_res_30.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Ktp1ggJES1d3"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "NC-_IprqQ8Qe"
      },
      "outputs": [],
      "source": [
        "class_res_30.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f9xJHJiRXMo",
        "outputId": "26b20d02-4949-4351-94a3-39a9077a3a0f"
      },
      "outputs": [],
      "source": [
        "earlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', # n s'arrête quand la loss est au plateau\n",
        "                                                         patience= EARLY_STOPPING_CRITERIA,\n",
        "                                                         verbose= 1 ,\n",
        "                                                         restore_best_weights=True\n",
        "                                                        )\n",
        "\n",
        "history = class_res_30.fit(x = train_generator,\n",
        "                    epochs = EPOCHS ,\n",
        "                    validation_data = validation_generator ,\n",
        "                    callbacks= [earlyStoppingCallback],\n",
        "                    class_weight=class_weight)\n",
        "\n",
        "his_res_1 = pd.DataFrame(history.history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "qBR6sB7onP38",
        "outputId": "4fc0a1a0-0c94-4ed3-dcf5-75f1e280f547"
      },
      "outputs": [],
      "source": [
        "plt.plot(his_res_1[\"categorical_accuracy\"], c=\"r\", label=\"train_accuracy\")\n",
        "plt.plot(his_res_1[\"val_categorical_accuracy\"], c=\"b\", label=\"test_accuracy\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "SL5U8779ndN4",
        "outputId": "1413952c-7113-4d30-d0e0-ed5007bcbc68"
      },
      "outputs": [],
      "source": [
        "plt.plot(his_res_1[\"loss\"], c=\"r\", label=\"train_loss\")\n",
        "plt.plot(his_res_1[\"val_loss\"], c=\"b\", label=\"test_loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "dScS1QDYfgz5"
      },
      "outputs": [],
      "source": [
        "class_res_1.save(\"/content/ProjetFinal_JedhaDS27_Emotions/class_res_1.h5\")\n",
        "his_res_1.to_csv('/content/ProjetFinal_JedhaDS27_Emotions/his_res_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oAx8exZEnDmA",
        "outputId": "b2704578-8aae-4374-aa52-0a14698ea121"
      },
      "outputs": [],
      "source": [
        "his_res_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A travailler au plus vite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classweight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Grégory\n",
        "\n",
        "Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "\n",
        "Weight for class 0: 0.50\n",
        "Weight for class 1: 289.44\n",
        "\n",
        "\n",
        " — \n",
        "Aujourd’hui à 14:49\n",
        "weighted_model = make_model()\n",
        "weighted_model.load_weights(initial_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight)\n",
        "\n",
        "print(img_generator_flow_train.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRAD CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GradCam\n",
        " Create an object `imgs` and an object `labels` containing a batch of validation images and validation labels.\n",
        "imgs, labels = next(iter(img_generator_flow_valid))\n",
        "\n",
        "\n",
        "2. Get the name of the last convolution layer of the pretrained model and the predicition layers and store them in two variables called respectively `last_conv_layer_name`, and `classifier_layer_names`.\n",
        "for layer in model.layers:\n",
        "  print(layer.name)\n",
        "\n",
        "base_model = model.layers[0]\n",
        "\n",
        "last_conv_layer_name = \"mixed10\"\n",
        "classifier_layer_names = [layer.name for layer in model.layers][1:]\n",
        "\n",
        "3. Run the following command, these functions will help you produce the grad cam\n",
        "\n",
        "# We start by setting up the dependencies we will use\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "# The Grad-CAM algorithm\n",
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.preprocessing.image.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(\n",
        "    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer\n",
        "    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n",
        "    last_conv_layer_model = keras.Model(base_model.inputs, last_conv_layer.output)\n",
        "\n",
        "    # Second, we create a model that maps the activations of the last conv\n",
        "    # layer to the final class predictions\n",
        "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "    for layer_name in classifier_layer_names:\n",
        "        x = model.get_layer(layer_name)(x)\n",
        "    classifier_model = keras.Model(classifier_input, x)\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute activations of the last conv layer and make the tape watch it\n",
        "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "        # Compute class predictions\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "        top_pred_index = tf.argmax(preds[0])\n",
        "        top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "    # This is the gradient of the top predicted class with regard to\n",
        "    # the output feature map of the last conv layer\n",
        "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    # The channel-wise mean of the resulting feature map\n",
        "    # is our heatmap of class activation\n",
        "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "4. Create a `preds` object containing the predicitons from your model on the batch of validation images.\n",
        "\n",
        "Then create a `pred_labels` object containing the top class predicted for each image in that batch. (You can use the `tf.argmax` function for this)\n",
        "\n",
        "# Print what the top predicted class is\n",
        "preds = model.predict(imgs)\n",
        "pred_labels = tf.argmax(preds, axis = -1)\n",
        "\n",
        "print(\"Prediction output:\", preds)\n",
        "\n",
        "print(\"Predicted label:\", pred_labels)\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmaps = []\n",
        "\n",
        "for img in imgs:\n",
        "  heatmap = make_gradcam_heatmap(\n",
        "    tf.expand_dims(img,axis=0),base_model, model, last_conv_layer_name, classifier_layer_names\n",
        "  )\n",
        "  heatmaps.append(heatmap)\n",
        "\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmaps[0])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "6. Loop over each image, predicted label and heatmap in order to display the images with the superimposed grad cam heatmap and the corresponding predicted label. Do they match the true label? What happens to the grad cam for wrong predictions? Are there any grad cams that seem surprising to you?\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "for img, pred_label, true_label, heatmap in zip(imgs, pred_labels, labels, heatmaps):\n",
        "  # We rescale heatmap to a range 0-255\n",
        "  heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "  # We use jet colormap to colorize heatmap\n",
        "  jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "  # We use RGB values of the colormap\n",
        "  jet_colors = jet(np.arange(256))[:, :3]\n",
        "  jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "  # We create an image with RGB colorized heatmap\n",
        "  jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "  jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "  jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "  # Superimpose the heatmap on original image\n",
        "  superimposed_img = jet_heatmap * 0.003 + img\n",
        "  superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "  # Save the superimposed image\n",
        "  save_path = \"saved_img.jpg\"\n",
        "  superimposed_img.save(save_path)\n",
        "\n",
        "  # Display Grad CAM\n",
        "  pred_file_path = np.argmax(img_generator_flow_valid.labels == pred_label)\n",
        "  pred_label_name = Path(img_generator_flow_valid.filepaths[pred_file_path]).parent.name\n",
        "\n",
        "  true_file_path = np.argmax(img_generator_flow_valid.labels == tf.argmax(true_label))\n",
        "  true_label_name = Path(img_generator_flow_valid.filepaths[true_file_path]).parent.name\n",
        "\n",
        "  print(\"Predicted label:\",pred_label_name)\n",
        "  print(\"True label:\", true_label_name)\n",
        "\n",
        "  display(Image(save_path))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
